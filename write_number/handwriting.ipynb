{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习 实验3\n",
    "\n",
    "**基于全链接网络的手写数字体识别**\n",
    "\n",
    "- 数据：MNIST data set\n",
    "- 本题目考察如何设计并实现一个简单的图像分类器，设置本题目的目的如下：\n",
    "1. 理解基本的图像识别流程的方法（预处理、训练、预测等阶段）\n",
    "2. 实现一个全连接神经网络分类器\n",
    "3. 理解不同的分类器之间的区别，以及使用不同的更新方法优化神经网络\n",
    "\n",
    "- 课后作业：\n",
    "1. 完成测试集上的测试过程\n",
    "\n",
    "- 附加题： \n",
    "1. 尝试使用不同的损失函数和正则化方法，观察并分析其对实验结果的影响 \n",
    "2. 尝试使用不同的优化算法，观察并分析其对训练过程和实验结果的影响， (如batch GD, online GD, mini-batch GD, SGD, 或其它的优化算法，如Momentum, Adsgrad, Adam, Admax)\n",
    "3. 增加训练的epoch，并绘制loss变化的图像\n",
    "4. 更改网络的结构，查看对训练过程额和最终结果有何影响\n",
    "\n",
    "- 补充：MINST是一个手写数字数据集，包括了若干手写数字体及其对应的数字，共60000个训练样本，10000个测试样本。每个手写数字被表示为一个28*28的向量。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 准备数据+数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import mnist  # 导入内置的 mnist 数据\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),   # 将图像转换为PyTorch的张量（tensor）格式，这是神经网络模型所需的数据格式\n",
    "    transforms.Normalize([0.5], [0.5])   # 如果不缩放，像素值范围较大可能会导致梯度爆炸或梯度消失等问题，从而影响模型的训练效果\n",
    "])\n",
    "\n",
    "# 使用mnist.MNIST()函数创建训练集、测试集两个数据集对象\n",
    "train_set = mnist.MNIST('./data copy', train=True, transform=transform, download=True)\n",
    "test_set = mnist.MNIST('./data copy', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 了解数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_data, a_label = train_set[0]\n",
    "a_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 创建数据加载器（DataLoader）对象\n",
    "\n",
    "用于在训练和测试神经网络模型时加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import* # 导入所有需要的类与函数\n",
    "\n",
    "train_data = DataLoader(train_set, batch_size=64,shuffle=True)\n",
    "test_data = DataLoader(test_set, batch_size=128,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FNN,self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(784,400),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(400,200),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(200,100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(100,10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        output = self.layer4(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 训练网络model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of FNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=10, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FNN()\n",
    "net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net,train_data,loss_func,optimizer,num_epochs):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for image,label in train_data:\n",
    "            image = image.view(image.size(0),-1)\n",
    "\n",
    "            out = net(image)\n",
    "            loss = loss_func(out,label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # get predicted results\n",
    "            _,predicted = torch.max(out.data,1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "\n",
    "        avg_loss = train_loss / len(train_data)\n",
    "        accuracy = correct / total\n",
    "\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "\n",
    "        print(f'epoch:{epoch}, Train loss：{avg_loss:.6f},Train Accuracy: {accuracy:.6f}')\n",
    "\n",
    "    return train_losses, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "loss_cross = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_sgd = torch.optim.SGD(net.parameters(),weight_decay=1e-3,lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, Train loss：1.270624,Train Accuracy: 0.661267\n",
      "epoch:1, Train loss：0.988138,Train Accuracy: 0.742067\n",
      "epoch:2, Train loss：0.804196,Train Accuracy: 0.779083\n",
      "epoch:3, Train loss：0.688883,Train Accuracy: 0.802833\n",
      "epoch:4, Train loss：0.613475,Train Accuracy: 0.820283\n",
      "epoch:5, Train loss：0.559467,Train Accuracy: 0.836083\n",
      "epoch:6, Train loss：0.517604,Train Accuracy: 0.849917\n",
      "epoch:7, Train loss：0.483484,Train Accuracy: 0.860817\n",
      "epoch:8, Train loss：0.454978,Train Accuracy: 0.868900\n",
      "epoch:9, Train loss：0.431787,Train Accuracy: 0.876283\n"
     ]
    }
   ],
   "source": [
    "losses_sgd, accs_sgd = train_model(net,train_data,loss_cross,optimizer_sgd,num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 尝试不同正则化、优化方法，并分析结果\n",
    "\n",
    "- 使用SGD随机梯度下降算法，尝试不同的L2正则化的参数lambda\n",
    "- 使用adam优化算法，正则化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义全连接神经网络\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation=nn.ReLU()):\n",
    "        \"\"\"\n",
    "        初始化全连接神经网络\n",
    "        \n",
    "        参数:\n",
    "            input_size (int): 输入特征的数量\n",
    "            hidden_sizes (list): 包含了每个隐藏层的神经元数量的列表\n",
    "            output_size (int): 输出的类别数量\n",
    "            activation (torch.nn.Module, optional): 激活函数，默认为ReLU\n",
    "        \"\"\"\n",
    "        super(FullyConnectedNN, self).__init__()\n",
    "        \n",
    "        # 步骤1: 创建输入层到第一个隐藏层的线性变换\n",
    "        self.input_layer = nn.Linear(input_size, hidden_sizes[0])\n",
    "        \n",
    "        # 步骤2: 创建隐藏层到隐藏层之间的线性变换和激活函数\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_sizes[i], hidden_sizes[i+1]),\n",
    "                activation\n",
    "            )\n",
    "            for i in range(len(hidden_sizes) - 1)\n",
    "        ])\n",
    "        \n",
    "        # 步骤3: 创建最后一个隐藏层到输出层的线性变换\n",
    "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        定义前向传播过程\n",
    "        \n",
    "        参数:\n",
    "            x (torch.Tensor): 输入数据张量，形状为 (batch_size, input_size)\n",
    "        \n",
    "        返回:\n",
    "            torch.Tensor: 输出数据张量，形状为 (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        # 步骤4: 输入数据通过输入层进行线性变换\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # 步骤5: 数据通过每个隐藏层的线性变换和激活函数进行传递\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "        \n",
    "        # 步骤6: 数据通过输出层的线性变换得到最终输出\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用不同优化算法、正则化参数训练\n",
    "\n",
    "# 定义训练函数\n",
    "def train(model, train_loader, test_loader, optimizer, criterion, num_epochs=5):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.view(images.size(0), -1)  # 展平图像为一维张量\n",
    "            optimizer.zero_grad()  # 梯度清零\n",
    "            outputs = model(images)  # 前向传播\n",
    "            loss = criterion(outputs, labels)  # 计算损失\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 参数更新\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted_train = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted_train == labels).sum().item()\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Evaluation on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images = images.view(images.size(0), -1)  # 展平图像为一维张量\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted_test = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted_test == labels).sum().item()\n",
    "\n",
    "        # 计算平均测试损失和准确率\n",
    "        test_losses.append(test_loss / len(test_loader))\n",
    "        test_accuracy = correct_test / total_test\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracy:.4f}, Test Loss: {test_losses[-1]:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 可视化不同优化算法、正则化参数的训练效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer:\n",
      "Epoch 1/10, Train Loss: 0.9087, Train Acc: 0.7598, Test Loss: 0.4053, Test Acc: 0.8864\n",
      "Epoch 2/10, Train Loss: 0.3738, Train Acc: 0.8929, Test Loss: 0.3260, Test Acc: 0.9030\n",
      "Epoch 3/10, Train Loss: 0.3230, Train Acc: 0.9054, Test Loss: 0.2985, Test Acc: 0.9120\n",
      "Epoch 4/10, Train Loss: 0.2984, Train Acc: 0.9126, Test Loss: 0.2826, Test Acc: 0.9151\n",
      "Epoch 5/10, Train Loss: 0.2798, Train Acc: 0.9182, Test Loss: 0.2668, Test Acc: 0.9222\n",
      "Epoch 6/10, Train Loss: 0.2640, Train Acc: 0.9223, Test Loss: 0.2582, Test Acc: 0.9219\n",
      "Epoch 7/10, Train Loss: 0.2502, Train Acc: 0.9271, Test Loss: 0.2522, Test Acc: 0.9256\n",
      "Epoch 8/10, Train Loss: 0.2368, Train Acc: 0.9311, Test Loss: 0.2329, Test Acc: 0.9302\n",
      "Epoch 9/10, Train Loss: 0.2245, Train Acc: 0.9346, Test Loss: 0.2140, Test Acc: 0.9368\n",
      "Epoch 10/10, Train Loss: 0.2132, Train Acc: 0.9375, Test Loss: 0.2119, Test Acc: 0.9387\n",
      "Eval Loss: 0.211888, Eval Acc: 0.938700\n",
      "Training with Adam optimizer:\n",
      "Epoch 1/10, Train Loss: 0.3825, Train Acc: 0.8853, Test Loss: 0.2127, Test Acc: 0.9396\n",
      "Epoch 2/10, Train Loss: 0.2013, Train Acc: 0.9388, Test Loss: 0.1576, Test Acc: 0.9520\n",
      "Epoch 3/10, Train Loss: 0.1681, Train Acc: 0.9489, Test Loss: 0.1509, Test Acc: 0.9509\n",
      "Epoch 4/10, Train Loss: 0.1486, Train Acc: 0.9547, Test Loss: 0.1398, Test Acc: 0.9539\n",
      "Epoch 5/10, Train Loss: 0.1370, Train Acc: 0.9574, Test Loss: 0.1685, Test Acc: 0.9460\n",
      "Epoch 6/10, Train Loss: 0.1317, Train Acc: 0.9593, Test Loss: 0.1230, Test Acc: 0.9620\n",
      "Epoch 7/10, Train Loss: 0.1234, Train Acc: 0.9616, Test Loss: 0.1162, Test Acc: 0.9637\n",
      "Epoch 8/10, Train Loss: 0.1177, Train Acc: 0.9637, Test Loss: 0.1170, Test Acc: 0.9655\n",
      "Epoch 9/10, Train Loss: 0.1152, Train Acc: 0.9639, Test Loss: 0.1301, Test Acc: 0.9586\n",
      "Epoch 10/10, Train Loss: 0.1119, Train Acc: 0.9654, Test Loss: 0.1195, Test Acc: 0.9620\n",
      "Eval Loss: 0.119527, Eval Acc: 0.962000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 数据预处理和加载\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "set1 = mnist.MNIST('./data', train=True, transform=transform, download=True)\n",
    "set2 = mnist.MNIST('./data', train=False, transform=transform, download=True)\n",
    "loader1 = DataLoader(set1, batch_size=64, shuffle=True)\n",
    "loader2 = DataLoader(set2, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "# 评估\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(images.size(0), -1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    eval_loss /= len(test_loader)\n",
    "    eval_acc = correct / total\n",
    "\n",
    "    print('Eval Loss: {:.6f}, Eval Acc: {:.6f}'.format(eval_loss, eval_acc))\n",
    "\n",
    "# 训练和评估\n",
    "input_size = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "# 使用SGD优化算法，无正则化\n",
    "model_sgd = FullyConnectedNN(input_size, hidden_sizes, output_size)\n",
    "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=0.01)\n",
    "criterion_sgd = nn.CrossEntropyLoss()\n",
    "print(\"Training with SGD optimizer:\")\n",
    "train_losses_sgd, test_losses_sgd, train_accuracies_sgd, test_accuracies_sgd = train(model_sgd, loader1, loader2, optimizer_sgd, criterion_sgd, num_epochs)\n",
    "evaluate(model_sgd, loader2, criterion_sgd)\n",
    "\n",
    "# 使用Adam优化算法，设置L2正则化参数为0.001\n",
    "model_adam = FullyConnectedNN(input_size, hidden_sizes, output_size)\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.001, weight_decay=0.001)\n",
    "criterion_adam = nn.CrossEntropyLoss()\n",
    "print(\"Training with Adam optimizer:\")\n",
    "train_losses_adam, test_losses_adam, train_accuracies_adam, test_accuracies_adam = train(model_adam, loader1, loader2, optimizer_adam, criterion_adam, num_epochs)\n",
    "evaluate(model_adam, loader2, criterion_adam)\n",
    "\n",
    "# 绘制损失曲线\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# 绘制损失曲线\n",
    "def plot_losses(train_losses, test_losses, title):\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 绘制训练和测试损失曲线\n",
    "plot_losses(train_losses_sgd, test_losses_sgd, 'SGD Optimizer Loss')\n",
    "plot_losses(train_losses_adam, test_losses_adam, 'Adam Optimizer Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 测试集上的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同学们将测试过程补充在这里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.264236, Eval Acc: 0.923500\n",
      "Eval Loss: 0.135656, Eval Acc: 0.958500\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_loader, criterion):\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()  # 将模型改为预测模式，不启用 BatchNormalization 和 Dropout\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.view(images.size(0), -1)  # 将图片展平为一维向量\n",
    "\n",
    "            # 前向传播计算loss\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 计算测试集上的总eval_loss\n",
    "            eval_loss += loss.item()\n",
    "            \n",
    "            # 得到预测结果\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # 计算预测正确的图片数量\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    eval_loss /= len(test_loader)\n",
    "    eval_acc = correct / total\n",
    "\n",
    "    print('Eval Loss: {:.6f}, Eval Acc: {:.6f}'.format(eval_loss, eval_acc))\n",
    "\n",
    "# 在训练完成后单独进行模型评估\n",
    "evaluate(model_sgd, loader2, criterion_sgd)\n",
    "evaluate(model_adam, loader2, criterion_adam)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
